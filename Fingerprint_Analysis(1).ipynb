{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e3548cf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNropOaaOg05"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Independent\\ Study/fingerprints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jPP0g4ROhZr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Path to concerned csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aab4e841"
      },
      "source": [
        "# Approach 1: Confusion Matrix Fingerprinting\n",
        "\n",
        "This approach treats each model's confusion matrix as a unique \"fingerprint\"\n",
        "\n",
        "that captures its systematic error patterns and prediction behavior.\n",
        "\n",
        "## Key Concepts:\n",
        "\n",
        "*   **Fingerprint**: 16-dimensional vector from flattened 4Ã—4 confusion matrix\n",
        "*   **Identification Method**: Cosine similarity matching\n",
        "*   **Goal**: Identify which model produced a set of predictions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "006e012f"
      },
      "source": [
        "**Cell 1 â€“ Setup and Imports**\n",
        "\n",
        "Initializes the Python environment, imports all required numerical, machine learning,\n",
        "\n",
        "and visualization libraries, and suppresses non-critical warnings to keep logs clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b084a8e0"
      },
      "outputs": [],
      "source": [
        "# Purpose: Import required libraries and configure the runtime environment.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d08cc91a"
      },
      "source": [
        "**Cell 2 â€“ Load Data and Extract Model Information**\n",
        "\n",
        "Automatically discovers all evaluated models by scanning prediction columns and\n",
        "\n",
        "prints a concise dataset summary. This avoids hardcoding model names and ensures\n",
        "\n",
        "scalability to larger model sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b7364cf"
      },
      "outputs": [],
      "source": [
        "# Purpose: Identify model prediction columns and summarize dataset structure.\n",
        "\n",
        "# Assuming df is already loaded\n",
        "\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Extract all model names from columns\n",
        "\n",
        "pred_cols = [col for col in df.columns if col.endswith('_pred')]\n",
        "model_names = [col.replace('_pred', '') for col in pred_cols]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Number of models: {len(model_names)}\")\n",
        "print(f\"\\nModels found:\")\n",
        "for i, name in enumerate(model_names, 1):\n",
        "    print(f\"  {i:2d}. {name}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bad1eab"
      },
      "source": [
        "**Cell 3 â€“ Data Quality Check**\n",
        "\n",
        "Validates prediction ranges for each model, quantifies invalid outputs, and reports\n",
        "\n",
        "per-model data hygiene to prevent corrupted or misleading fingerprints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4d8a36d"
      },
      "outputs": [],
      "source": [
        "# Purpose: Verify prediction validity and quantify invalid label occurrences.\n",
        "\n",
        "# Create model prediction columns dictionary\n",
        "\n",
        "model_pred_cols = {name: f\"{name}_pred\" for name in model_names}\n",
        "model_correct_cols = {name: f\"{name}_correct\" for name in model_names}\n",
        "\n",
        "n_classes = 4\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Model':<30s} {'Invalid':<10s} {'Valid':<10s} {'Invalid %':<10s}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "quality_report = []\n",
        "for name in model_names:\n",
        "    col = model_pred_cols[name]\n",
        "    n_invalid = ((df[col] < 0) | (df[col] >= n_classes)).sum()\n",
        "    n_valid = len(df) - n_invalid\n",
        "    pct_invalid = 100 * n_invalid / len(df)\n",
        "    quality_report.append({\n",
        "        'model': name,\n",
        "        'invalid': n_invalid,\n",
        "        'valid': n_valid,\n",
        "        'pct_invalid': pct_invalid\n",
        "    })\n",
        "    print(f\"{name:<30s} {n_invalid:<10d} {n_valid:<10d} {pct_invalid:<10.2f}%\")\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5856bd0b"
      },
      "source": [
        "**Cell 4 â€“ Train/Test Split**\n",
        "\n",
        "Splits the dataset into disjoint training and test partitions. Fingerprints are\n",
        "\n",
        "constructed exclusively on the training set to prevent information leakage during\n",
        "\n",
        "identification evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9daa6cc8"
      },
      "outputs": [],
      "source": [
        "# Purpose: Partition data to prevent fingerprint leakage during evaluation.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Split data into train (80%) and test (20%) sets\n",
        "\n",
        "# IMPORTANT: We'll compute fingerprints ONLY on training data to avoid leakage\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3121ee44"
      },
      "source": [
        "**Cell 5 â€“ Confusion Matrix Fingerprinting Function**\n",
        "\n",
        "Defines the core fingerprint primitive: a normalized confusion histogram that captures\n",
        "\n",
        "systematic prediction tendencies and structured error behavior for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6c4470a"
      },
      "outputs": [],
      "source": [
        "# Purpose: Define the core transformation from predictions to fingerprint vectors.\n",
        "\n",
        "def confusion_hist(gold_labels, pred_labels, n_classes=4):\n",
        "    \"\"\"\n",
        "    Compute a confusion matrix and return it as a normalized histogram.\n",
        "    This creates a 'fingerprint' of a model's error patterns.\n",
        "\n",
        "    Args:\n",
        "        gold_labels: Ground truth labels\n",
        "        pred_labels: Model predictions\n",
        "        n_classes: Number of classes (4 for HellaSwag)\n",
        "\n",
        "    Returns:\n",
        "        v: Flattened, normalized confusion histogram (16-d for 4 classes)\n",
        "        conf: Raw confusion matrix (4x4)\n",
        "    \"\"\"\n",
        "    # Initialize confusion matrix\n",
        "    conf = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
        "\n",
        "    # Populate confusion matrix\n",
        "    # conf[i,j] = number of times true label was i and prediction was j\n",
        "    for g, p in zip(gold_labels, pred_labels):\n",
        "        if 0 <= g < n_classes and 0 <= p < n_classes:\n",
        "            conf[g, p] += 1\n",
        "\n",
        "    # Flatten and normalize to create a probability distribution\n",
        "    v = conf.flatten().astype(np.float32)\n",
        "    v /= v.sum() + 1e-8  # Add epsilon to avoid division by zero\n",
        "\n",
        "    return v, conf\n",
        "\n",
        "print(\"âœ“ Confusion fingerprinting function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd015d92"
      },
      "source": [
        "**Cell 6 â€“ Compute Model Fingerprints**\n",
        "\n",
        "Computes one reference fingerprint per model using only training data and reports\n",
        "\n",
        "baseline accuracies to contextualize fingerprint quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5355ce"
      },
      "outputs": [],
      "source": [
        "# Purpose: Generate reference fingerprints for each model using training data.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPUTING MODEL FINGERPRINTS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"(Using training data only to avoid leakage)\\n\")\n",
        "\n",
        "train_gold = train_df[\"label\"].to_numpy()\n",
        "\n",
        "fingerprints = {}  # Normalized 16-d vectors\n",
        "conf_mats = {}     # Raw 4x4 confusion matrices\n",
        "\n",
        "for name in model_names:\n",
        "    col = model_pred_cols[name]\n",
        "    train_pred = train_df[col].to_numpy()\n",
        "\n",
        "    v, cm = confusion_hist(train_gold, train_pred, n_classes)\n",
        "    fingerprints[name] = v\n",
        "    conf_mats[name] = cm\n",
        "\n",
        "    acc = cm.diagonal().sum() / cm.sum() if cm.sum() > 0 else 0\n",
        "    print(f\"{name:<30s} Accuracy: {acc:.3f}\")\n",
        "\n",
        "print(f\"\\nâœ“ Fingerprints computed for {len(fingerprints)} models\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27650d9a"
      },
      "source": [
        "**Cell 7 â€“ Visualize Individual Confusion Matrices**\n",
        "\n",
        "Displays normalized confusion matrices for a subset of models to enable qualitative\n",
        "\n",
        "inspection of model-specific error patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d5724d"
      },
      "outputs": [],
      "source": [
        "# Purpose: Qualitatively inspect per-model error patterns.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VISUALIZING CONFUSION MATRICES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show first 6 models as example\n",
        "\n",
        "n_display = min(6, len(model_names))\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, name in enumerate(model_names[:n_display]):\n",
        "    cm = conf_mats[name]\n",
        "\n",
        "    # Normalize by row (true label) to show distribution of predictions\n",
        "    cm_norm = cm.astype(float)\n",
        "    row_sums = cm_norm.sum(axis=1, keepdims=True)\n",
        "    cm_norm = cm_norm / (row_sums + 1e-8)\n",
        "\n",
        "    ax = axes[idx]\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                ax=ax, cbar=True, vmin=0, vmax=1)\n",
        "\n",
        "    acc = cm.diagonal().sum()/cm.sum() if cm.sum() > 0 else 0\n",
        "    ax.set_title(f'{name}\\nAcc: {acc:.3f}', fontsize=11)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices_sample.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ“ Saved: confusion_matrices_sample.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83b4ddad"
      },
      "source": [
        "**Cell 8 â€“ Complete Confusion Matrix Grid**\n",
        "\n",
        "Visualizes confusion matrices for all models simultaneously, facilitating comparison\n",
        "\n",
        "and revealing structural similarities across model families."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ccbf32"
      },
      "outputs": [],
      "source": [
        "# Purpose: Visualize confusion matrices for all models at once.\n",
        "\n",
        "n_models = len(model_names)\n",
        "n_cols = 4\n",
        "n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
        "axes = axes.flatten() if n_models > 1 else [axes]\n",
        "\n",
        "for idx, name in enumerate(model_names):\n",
        "    cm = conf_mats[name]\n",
        "\n",
        "    # Normalize by row\n",
        "    cm_norm = cm.astype(float)\n",
        "    row_sums = cm_norm.sum(axis=1, keepdims=True)\n",
        "    cm_norm = cm_norm / (row_sums + 1e-8)\n",
        "\n",
        "    ax = axes[idx]\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                ax=ax, cbar=True, vmin=0, vmax=1)\n",
        "\n",
        "    acc = cm.diagonal().sum()/cm.sum() if cm.sum() > 0 else 0\n",
        "    ax.set_title(f'{name}\\nAcc: {acc:.3f}', fontsize=10)\n",
        "    ax.set_xlabel('Predicted', fontsize=9)\n",
        "    ax.set_ylabel('True', fontsize=9)\n",
        "\n",
        "# Hide unused subplots\n",
        "\n",
        "for idx in range(n_models, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices_all.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ“ Saved: confusion_matrices_all.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ba9e623"
      },
      "source": [
        "**Cell 9 â€“ Fingerprint Similarity Analysis**\n",
        "\n",
        "Computes pairwise cosine similarity between fingerprints to quantify behavioral\n",
        "\n",
        "proximity and divergence among models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "436a5296"
      },
      "outputs": [],
      "source": [
        "# Purpose: Quantify similarity between model fingerprints via cosine similarity.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINGERPRINT SIMILARITY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Stack all reference fingerprints into a matrix\n",
        "\n",
        "family_matrix = np.stack([fingerprints[name] for name in model_names])\n",
        "\n",
        "# Compute pairwise cosine similarities\n",
        "\n",
        "similarity_matrix = cosine_similarity(family_matrix)\n",
        "\n",
        "# Find most similar and most different pairs\n",
        "\n",
        "sim_pairs = []\n",
        "for i in range(len(model_names)):\n",
        "    for j in range(i+1, len(model_names)):\n",
        "        sim_pairs.append((model_names[i], model_names[j], similarity_matrix[i, j]))\n",
        "\n",
        "sim_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nMost Similar Model Pairs:\")\n",
        "for name1, name2, sim in sim_pairs[:5]:\n",
        "    print(f\"  {name1} <-> {name2}: {sim:.4f}\")\n",
        "\n",
        "print(\"\\nMost Different Model Pairs:\")\n",
        "for name1, name2, sim in sim_pairs[-5:]:\n",
        "    print(f\"  {name1} <-> {name2}: {sim:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Visualize similarity matrix\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='RdYlGn',\n",
        "            xticklabels=model_names, yticklabels=model_names,\n",
        "            vmin=0, vmax=1, center=0.5, linewidths=0.5)\n",
        "plt.title('Confusion Fingerprint Similarity Between Models\\n(Cosine Similarity)',\n",
        "          fontsize=16, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('fingerprint_similarity.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ“ Saved: fingerprint_similarity.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b064620c"
      },
      "source": [
        "**Cell 10 â€“ Model Identification System**\n",
        "\n",
        "Implements the inference-time identification procedure that matches unknown prediction\n",
        "\n",
        "sets to the closest reference fingerprint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c9f8a54"
      },
      "outputs": [],
      "source": [
        "# Purpose: Define the inference-time model identification procedure.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL IDENTIFICATION SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get test set ground truth\n",
        "\n",
        "test_gold = test_df[\"label\"].to_numpy()\n",
        "\n",
        "def identify_family(sub_gold, sub_pred):\n",
        "    \"\"\"\n",
        "    Identify which model family produced these predictions.\n",
        "\n",
        "    Method:\n",
        "    1. Compute confusion fingerprint for the subset\n",
        "    2. Calculate cosine similarity with each reference fingerprint\n",
        "    3. Return the model with highest similarity\n",
        "    \"\"\"\n",
        "    v, _ = confusion_hist(sub_gold, sub_pred, n_classes)\n",
        "    v = v.reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity with each reference fingerprint\n",
        "    sims = cosine_similarity(v, family_matrix)[0]\n",
        "    best_idx = np.argmax(sims)\n",
        "\n",
        "    return model_names[best_idx], sims\n",
        "\n",
        "print(\"âœ“ Model identification system ready\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50003b61"
      },
      "source": [
        "**Cell 11 â€“ Single-Model Identification Demonstration**\n",
        "\n",
        "Demonstrates identification behavior as a function of probe size using a fixed target\n",
        "\n",
        "model to illustrate robustness under limited observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b176f75"
      },
      "outputs": [],
      "source": [
        "# Purpose: Demonstrate identification behavior as probe size increases.\n",
        "\n",
        "print(\"Testing identification with different probe sizes...\\n\")\n",
        "\n",
        "test_model = model_names[0]\n",
        "test_col = model_pred_cols[test_model]\n",
        "test_pred = test_df[test_col].to_numpy()\n",
        "\n",
        "# Test with increasing probe sizes\n",
        "\n",
        "for n_probe in [5, 10, 20, 50, 100]:\n",
        "    if n_probe <= len(test_df):\n",
        "        idx = np.random.choice(len(test_df), size=n_probe, replace=False)\n",
        "        sub_gold = test_gold[idx]\n",
        "        sub_pred = test_pred[idx]\n",
        "\n",
        "        identified, sims = identify_family(sub_gold, sub_pred)\n",
        "\n",
        "        print(f\"With {n_probe:3d} probes from {test_model}:\")\n",
        "        print(f\"  Identified as: {identified}\")\n",
        "        print(f\"  Correct: {identified == test_model}\")\n",
        "        print(f\"  Confidence: {sims.max():.4f}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bdadf1a"
      },
      "source": [
        "**Cell 12 â€“ Comprehensive Probe Size Experiments**\n",
        "\n",
        "Systematically evaluates identification accuracy across probe sizes and random trials\n",
        "\n",
        "to estimate sample complexity for reliable model identification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "513cd633"
      },
      "outputs": [],
      "source": [
        "# Purpose: Systematically evaluate identification accuracy vs. probe size.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PROBE SIZE EXPERIMENTS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Evaluating identification accuracy vs. probe set size...\\n\")\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def eval_probe_size(model_name, n_probe=20, n_trials=100):\n",
        "    \"\"\"\n",
        "    Evaluate how many samples are needed to identify a model.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    col = model_pred_cols[model_name]\n",
        "    full_test_pred = test_df[col].to_numpy()\n",
        "    n_items_test = len(test_df)\n",
        "\n",
        "    for _ in range(n_trials):\n",
        "        # Sample random subset of test predictions\n",
        "        idx = rng.choice(n_items_test, size=min(n_probe, n_items_test), replace=False)\n",
        "        sub_gold = test_gold[idx]\n",
        "        sub_pred = full_test_pred[idx]\n",
        "\n",
        "        # Identify model based on this subset\n",
        "        identified_family, _ = identify_family(sub_gold, sub_pred)\n",
        "        if identified_family == model_name:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / n_trials\n",
        "\n",
        "# Test various probe sizes\n",
        "\n",
        "probe_sizes = [5, 10, 20, 50, 100]\n",
        "results = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    results[model_name] = []\n",
        "    print(f\"{model_name}:\")\n",
        "\n",
        "    for n_probe in probe_sizes:\n",
        "        acc = eval_probe_size(model_name, n_probe=n_probe, n_trials=100)\n",
        "        results[model_name].append(acc)\n",
        "        print(f\"  {n_probe:3d} probes: {acc:.3f} accuracy\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b19ad3"
      },
      "source": [
        "**Cell 13 â€“ Probe Size Results Visualization**\n",
        "\n",
        "Plots identification accuracy versus probe size to characterize convergence behavior\n",
        "\n",
        "and inter-model variability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ff07211"
      },
      "outputs": [],
      "source": [
        "# Purpose: Plot identification accuracy as a function of probe size.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VISUALIZING PROBE SIZE RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "for model_name, accs in results.items():\n",
        "    plt.plot(probe_sizes, accs, marker='o', label=model_name,\n",
        "             linewidth=2, markersize=6)\n",
        "\n",
        "plt.xlabel('Number of Probe Samples', fontsize=14)\n",
        "plt.ylabel('Identification Accuracy', fontsize=14)\n",
        "plt.title('Model Identification Accuracy vs. Probe Set Size',\n",
        "          fontsize=16, fontweight='bold')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.tight_layout()\n",
        "plt.savefig('probe_size_accuracy.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ“ Saved: probe_size_accuracy.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acba3e92"
      },
      "source": [
        "**Cell 14 â€“ Performance vs. Identifiability Analysis**\n",
        "\n",
        "Examines the relationship between task accuracy and fingerprint identifiability,\n",
        "\n",
        "including correlation analysis to assess dependence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "870b1988"
      },
      "outputs": [],
      "source": [
        "# Purpose: Relate model accuracy to identifiability.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"IDENTIFICATION ACCURACY BY MODEL PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get model accuracies\n",
        "\n",
        "accuracies = {}\n",
        "for name in model_names:\n",
        "    col = model_pred_cols[name]\n",
        "    test_pred = test_df[col].to_numpy()\n",
        "    mask = (test_pred >= 0) & (test_pred < n_classes)\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        acc = (test_gold[mask] == test_pred[mask]).mean()\n",
        "        accuracies[name] = acc\n",
        "\n",
        "# Create scatter plot: model accuracy vs identification accuracy\n",
        "\n",
        "acc_20 = [results[name][2] for name in model_names]  # 20-probe accuracy\n",
        "model_accs = [accuracies[name] for name in model_names]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(model_accs, acc_20, s=200, alpha=0.6, edgecolors='black', linewidth=2)\n",
        "\n",
        "for i, name in enumerate(model_names):\n",
        "    plt.annotate(name, (model_accs[i], acc_20[i]),\n",
        "                 xytext=(5, 5), textcoords='offset points',\n",
        "                 fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.xlabel('Model Accuracy on Test Set', fontsize=14)\n",
        "plt.ylabel('Identification Accuracy (20 probes)', fontsize=14)\n",
        "plt.title('Model Performance vs. Identifiability', fontsize=16, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('performance_vs_identifiability.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ“ Saved: performance_vs_identifiability.png\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "pearson_r, pearson_p = pearsonr(model_accs, acc_20)\n",
        "spearman_r, spearman_p = spearmanr(model_accs, acc_20)\n",
        "\n",
        "print(f\"\\nCorrelation Analysis:\")\n",
        "print(f\"  Pearson correlation:  r={pearson_r:.3f}, p={pearson_p:.4f}\")\n",
        "print(f\"  Spearman correlation: r={spearman_r:.3f}, p={spearman_p:.4f}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdec10b9"
      },
      "source": [
        "**Cell 15 â€“ Confusion Structure Analysis**\n",
        "\n",
        "Analyzes off-diagonal confusion mass to quantify how dispersed each modelâ€™s error\n",
        "\n",
        "distribution is, providing insight into fingerprint distinctiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beae7a3c"
      },
      "outputs": [],
      "source": [
        "# Purpose: Compare models by distribution of systematic errors.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFUSION MATRIX DISTANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Compute average off-diagonal mass (error distribution)\n",
        "\n",
        "off_diag_mass = {}\n",
        "for name in model_names:\n",
        "    cm = conf_mats[name]\n",
        "    cm_norm = cm / (cm.sum() + 1e-8)\n",
        "    off_diag = cm_norm.sum() - cm_norm.diagonal().sum()\n",
        "    off_diag_mass[name] = off_diag\n",
        "\n",
        "# Sort models by error distribution\n",
        "\n",
        "sorted_models = sorted(off_diag_mass.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nModels by Error Distribution (off-diagonal mass):\")\n",
        "print(f\"{'Model':<30s} {'Error Mass':<12s} {'Accuracy':<10s}\")\n",
        "print(\"-\" * 52)\n",
        "for name, mass in sorted_models:\n",
        "    acc = accuracies.get(name, 0)\n",
        "    print(f\"{name:<30s} {mass:<12.4f} {acc:<10.3f}\")\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac78b808"
      },
      "source": [
        "**Cell 16 â€“ Summary Statistics**\n",
        "\n",
        "Aggregates key performance and identification metrics into concise summaries,\n",
        "\n",
        "highlighting best- and worst-case identifiability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d2d15a3"
      },
      "outputs": [],
      "source": [
        "# Purpose: Summarize identification and performance results.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nðŸ“Š MODEL ACCURACIES:\")\n",
        "print(f\"{'Model':<30s} {'Accuracy':<10s}\")\n",
        "print(\"-\" * 40)\n",
        "for name in sorted(accuracies.keys(), key=lambda x: accuracies[x], reverse=True):\n",
        "    print(f\"{name:<30s} {accuracies[name]:.3f}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ IDENTIFICATION PERFORMANCE:\")\n",
        "avg_acc_5 = np.mean([results[m][0] for m in model_names])\n",
        "avg_acc_10 = np.mean([results[m][1] for m in model_names])\n",
        "avg_acc_20 = np.mean([results[m][2] for m in model_names])\n",
        "avg_acc_50 = np.mean([results[m][3] for m in model_names])\n",
        "avg_acc_100 = np.mean([results[m][4] for m in model_names])\n",
        "\n",
        "print(f\"  With 5 probes:   {avg_acc_5:.3f} average accuracy\")\n",
        "print(f\"  With 10 probes:  {avg_acc_10:.3f} average accuracy\")\n",
        "print(f\"  With 20 probes:  {avg_acc_20:.3f} average accuracy\")\n",
        "print(f\"  With 50 probes:  {avg_acc_50:.3f} average accuracy\")\n",
        "print(f\"  With 100 probes: {avg_acc_100:.3f} average accuracy\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ BEST/WORST IDENTIFIABLE MODELS:\")\n",
        "id_20 = {name: results[name][2] for name in model_names}\n",
        "sorted_id = sorted(id_20.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"  Most identifiable:  {sorted_id[0][0]:30s} ({sorted_id[0][1]:.3f})\")\n",
        "print(f\"  Least identifiable: {sorted_id[-1][0]:30s} ({sorted_id[-1][1]:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… APPROACH 1 ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bed588b0"
      },
      "source": [
        "**Cell 17 â€“ Export Results**\n",
        "\n",
        "Persists all core metrics and identification results to disk to support reproducibility\n",
        "\n",
        "and downstream analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "328ee916"
      },
      "outputs": [],
      "source": [
        "# Purpose: Persist summary metrics for downstream analysis.\n",
        "\n",
        "# Create results summary dataframe\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'model': model_names,\n",
        "    'accuracy': [accuracies.get(name, 0) for name in model_names],\n",
        "    'id_5_probes': [results[name][0] for name in model_names],\n",
        "    'id_10_probes': [results[name][1] for name in model_names],\n",
        "    'id_20_probes': [results[name][2] for name in model_names],\n",
        "    'id_50_probes': [results[name][3] for name in model_names],\n",
        "    'id_100_probes': [results[name][4] for name in model_names],\n",
        "    'off_diagonal_mass': [off_diag_mass[name] for name in model_names]\n",
        "})\n",
        "\n",
        "results_df = results_df.sort_values('accuracy', ascending=False)\n",
        "results_df.to_csv('approach1_results.csv', index=False)\n",
        "print(\"âœ“ Saved: approach1_results.csv\")\n",
        "print(\"\\nResults DataFrame:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY7i7_dzYRYm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}